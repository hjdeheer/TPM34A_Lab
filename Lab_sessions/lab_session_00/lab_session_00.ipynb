{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPM034A Machine Learning for socio-technical systems \n",
    "## `Lab session 00: Basics of python and data analysis`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**Q2 2022**<br>\n",
    "**Instructor:** Sander van Cranenburgh <br>\n",
    "**TAs:**  Francisco Garrido Valenzuela & Lucas Spierenburg <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Instructions`\n",
    "\n",
    "**Lab session aim to:**<br>\n",
    "* Make you familiar with Python, and various widely used packages, such as Pandas, NumPy, and SciPy.<br>\n",
    "* Help you gather hands-on data analysis and machine learning skills.<br>\n",
    "\n",
    "\n",
    "**Lab sessions are:**<br>\n",
    "* Learning environments where you work with Jupyter notebooks and where you can get support from TAs and fellow students.<br> \n",
    "* Not graded and do not have to be submitted. \n",
    "* A good preparation for the **assignments** (which are graded).\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Workspace set-up`\n",
    "**Option 1: Google Colab**<br>\n",
    "Uncomment the following cells code lines if you are running this notebook on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/TPM34A/Q2_2022\n",
    "#!pip install -r Q2_2022/requirements_colab.txt\n",
    "#!mv \"/content/Q2_2022/Lab_sessions/lab_session_00/data\" /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Local environment**<br>\n",
    "Uncomment the following cell if you are running this notebook on your local environment. This will install all dependencies on your Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1. Python basics: built-in variables, data structures and functions` <br>\n",
    "\n",
    "#### 1.1 Built-in variables and operators\n",
    "Python is a programming language that works with three **basic** types of variables: **Numeric**, **Text** and **Boolean**. For each variable type, different operators are used to interact with other variables. The following list summarises the basic data types and the table summarizes the most common operators used in Python.\n",
    "\n",
    "- Numeric types -> int, float \n",
    "- Text types -> str\n",
    "- Boolean types -> bool <br>\n",
    "\n",
    "Where \"int\", \"float\", and \"str\" stand for \"integer\", \"floating point\" and \"string\"<br>\n",
    "Note that a Boolean can take only two values: \"True\" of \"False\"<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "| Operator  |   Name         |                              \n",
    "|-----------|----------------------------------------------|                               \n",
    "|   +   \t| Addition (for, int or float), or Concatenation (for, str)|                               \n",
    "|   -\t    | Subtraction    |                               \n",
    "|   *\t    | Multiplication |                               \n",
    "|   /\t    | Division\t     |                                    \n",
    "|   %\t    | Modulus\t     |                     \n",
    "|   **\t    | Exponentiation |                     \n",
    "|   //\t    | Floor division |\n",
    "\n",
    "Python also has comparison, assignment and logic operators. If you are interested go to this [link](https://www.w3schools.com/python/python_operators.asp)<br>\n",
    "\n",
    "### <span style=\"color:skyblue\">Playground 1: Basic operations and printing</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking the sum of two integers:\n",
      "\t a + b = 7\n",
      "Taking the sum of three floats:\n",
      "\t d + e + f = 11.3\n",
      "Taking concatifons (same operation \"+\" for integers) it results:\n",
      "\t h + i = This is not a integer, is text: 4 \n"
     ]
    }
   ],
   "source": [
    "# Basic operations \n",
    "a = 3                    # the variable 'a' takes int as its type (based on assigned value) \n",
    "b = int(4)               # the type can be explicitly defined by the built-in function 'int()'\n",
    "c = a + b                # a sum operation is computed with the values of 'a' and 'b', and the assigned to 'c'\n",
    "print(f'Taking the sum of two integers:\\n\\t a + b = {c}')\n",
    "\n",
    "d = 3.1                  # the variable 'd' takes float as its type (based on assigned value) \n",
    "e = float(4.2)           # the type can be explicitly defined by the built-in function 'float()'\n",
    "f = float(4)             # by default the type of 4 is int, but we assign an specific type id is needed (see the exercise below)\n",
    "g = d + e + f            # a sum operation is computed with the values of 'd', 'e', and 'f', the assigned to 'g'\n",
    "print(f'Taking the sum of three floats:\\n\\t d + e + f = {g}')\n",
    "\n",
    "h = 'This is not a integer, is text: '    # the variable 'h' takes str as its type (based on assigned value) \n",
    "i = str(4)                                # the type can be explicitly defined by the built-in function 'str()'. The value 4 is considered as text\n",
    "j = h + i                                 # a concatenation operation is computed with the values of 'h' and 'i', the assigned to 'j'\n",
    "print(f'Taking concatifons (same operation \"+\" for integers) it results:\\n\\t h + i = {j} ')\n",
    "\n",
    "# Data types cannot always be mixed!\n",
    "# Uncomment to see strings and numeric cannot be operated with + without type conversion\n",
    "# m = a + g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing with formats\n",
    "\n",
    "# Printing of (intermediate) results is often used by analysts to understand and debug their code\n",
    "# f-strings are a convenient way to print results in Pyhton, in which different data types can be combined:\n",
    "# You only have to add a 'f' in front of the string and add the numeric values in the bracket {}.\n",
    "pi = float(3.141592653589793238462643383279502884197)\n",
    "string_to_print = f'The number of pi = {pi}'\n",
    "print(string_to_print)\n",
    "\n",
    "# Using f-strings we can also control the formating of decimal on floats\n",
    "# The structure if 'value:width.decimals' and then add a f. for example: the value of pi with 4 spaces long, and 2 decimals should be:\n",
    "# 'value:width.decimals'f -> pi:4.2f \n",
    "\n",
    "# Let's print pi as a floating point with 2 digits behind the decimal separator\n",
    "string_to_print = f'The number of pi = {pi:4.2f}'\n",
    "print(string_to_print)\n",
    "\n",
    "# Let's print pi as a floating point with 5 digits behind the decimal separator\n",
    "string_to_print = f'The number of pi = {pi:4.5f}'\n",
    "print(string_to_print)\n",
    "\n",
    "# Let's print pi as a floating point with 10 field width, and with 5 digits behind the decimal separator\n",
    "string_to_print = f'The number of pi = {pi:10.5f}'\n",
    "print(string_to_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 1: Basic operations</span> \n",
    "\n",
    "Build a little code to do division between two numbers, get the quotient, integer quotient, and the remainder. Then make a \"pretty\" printout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excercise 1\n",
    "\n",
    "#dividend = \n",
    "#divisor =\n",
    "\n",
    "#quotient = \n",
    "#int_quotient =\n",
    "#remainder =\n",
    "\n",
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Built-in data structures\n",
    "\n",
    "A data structure is a collection of different types of data on which specific sets of operations can be performed. It is also used as a way to organise and store data. There are many types of data structures. The one you want to use depends on the particular needs you have. Below, you will see several widely used data structures.\n",
    "\n",
    "**a.- Lists and Tuples**\n",
    "\n",
    "**Lists** and **Tuples** are data structures that are used to manage data in an **orderly format**. The contents can be accessed using the **index** corresponding to the order in which the contents appear, as shown in the figure below. Tuples and Lists can contain different data types or objects. The main difference between Lists and Tuples is that Tuples cannot be modified after its creation, while Lists can be modified, such as e.g. by editing elements, removing elements, and adding new elements. \n",
    "\n",
    "- Lists use squared parentesis **[]**\n",
    "- Tuples use rounded parentesis **()**\n",
    "\n",
    "**b.- Dictionaries**\n",
    "\n",
    "Dictionaries are data structures for storing variables in **no particular order**. Unlike lists and tuples, which are indexed by row and column numbers, dictionaries are **indexed by keys**. A key is understood as a unique identifier that can be a number or a text.\n",
    "\n",
    "- Dictionaries use key parentesis **{}**\n",
    "\n",
    "### <span style=\"color:skyblue\">Playground 2: Built-in data structures</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists\n",
    "\n",
    "# Defining a list\n",
    "this_is_an_empty_list = list()\n",
    "this_is_also_an_empty_list = []\n",
    "this_is_a_list = [4, 5, 'this is text', True]\n",
    "print(f'The original list is: {this_is_a_list}')\n",
    "\n",
    "# Accessing to elements\n",
    "result1 = this_is_a_list[1]\n",
    "print(f'The value in index 1 of list \"this_is_a_list\" is: {result1}')\n",
    "\n",
    "# Adding elements\n",
    "this_is_a_list.append(76)\n",
    "print(f'After appending 76 to \"this_is_a_list\" is: {this_is_a_list}')\n",
    "\n",
    "# Modifiying an element\n",
    "this_is_a_list[2] = 'hello'\n",
    "print(f'After editing the third element (index == 2): {this_is_a_list}')\n",
    "\n",
    "# Removing the first element of the list\n",
    "this_is_a_list.pop(0)\n",
    "print(f'After deleting the first element: {this_is_a_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuples\n",
    "\n",
    "# Defining a tuple\n",
    "this_is_an_empty_tuple = tuple()\n",
    "this_is_a_tuple = (3, True, 'Hello')\n",
    "print(f'This is a tuple: {this_is_a_tuple}')\n",
    "\n",
    "# Accessing to elements\n",
    "result2 = this_is_a_tuple[1]\n",
    "print(f'The value in index 1 of tuple is: {result2}')\n",
    "\n",
    "# Uncomment the following line to see the error. What is happening?\n",
    "# this_is_a_tuple[1] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries\n",
    "\n",
    "# Defining a dictionary\n",
    "this_is_an_empty_dict = {}\n",
    "birds = {'pn': 'Penguins', 'eg': 'Eagle', 'sg': 'seagull'}\n",
    "print(f\"The original bird dictionary is: {birds}\")\n",
    "\n",
    "# Accessing to dict element\n",
    "result3 = birds['eg']\n",
    "print(f\"The bird with key 'eg' in dict is: {result3}\")\n",
    "\n",
    "# Adding elements\n",
    "birds.update({'dc': 'duck'})\n",
    "print(f\"The new bird dictionary after adding one element is: {birds}\")\n",
    "\n",
    "# Deleting elements\n",
    "del birds['pn']\n",
    "print(f\"The new bird dictionary after deleting one element is: {birds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 2: Built-in data structures</span> \n",
    "\n",
    "Build a small code where you create a dictionary with two elements:\n",
    "- Each element must be a list.\n",
    "- Then add one element to one of the list and delete an element in the other one.\n",
    "- Finally, add a third element to the dict with the concatenation of the first two lists (hint: strings can also work as lists).\n",
    "\n",
    "Print the final dict to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excercise 2\n",
    "\n",
    "#dict = \n",
    "\n",
    "# Add one element\n",
    "\n",
    "# Remove one element\n",
    "\n",
    "# Creates a new element\n",
    "\n",
    "# Print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Functions\n",
    "\n",
    "A function is a block of code that executes when it is called. A function can receive input, known as parameters and usually return one or multiple outputs (the results).\n",
    "\n",
    "### <span style=\"color:skyblue\">Playground 3: Functions</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# Definition of a function to sum two elements\n",
    "def sum_of_numbers(num1, num2):\n",
    "    result = num1 + num2\n",
    "    return result\n",
    "\n",
    "# Using the previous function\n",
    "n1 = 8\n",
    "n2 = 10\n",
    "resp = sum_of_numbers(n1, n2)\n",
    "print(f'The result of apply sum_of_numbers() is {resp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a method to compute the volumne of an sphere with certain radious\n",
    "# rounded to two decimals\n",
    "def sphere_vol(radius):\n",
    "    vol = 4/3*3.14*radius**3\n",
    "    vol_rounded = round(vol, 2)\n",
    "    return vol_rounded\n",
    "\n",
    "# Using the previous function\n",
    "r = 12\n",
    "vol = sphere_vol(12)\n",
    "print(f'The volumen of a sphere with radius {r}m is {vol}m3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 3: Functions</span> \n",
    "\n",
    "A. Create your own function that computes the mean across of the inputs<br> \n",
    "B. Create your own function that computes both the mean and the sum of the inputs<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `2. Data analysis: external libraries, Pandas and preliminary analysis` <br>\n",
    "\n",
    "#### 2.1 External libraries\n",
    "\n",
    "The following list shows useful Python libraries (library coding name in brackets) to work with data.\n",
    "\n",
    "- NumPy (numpy): is a scientific calculation library. It contains, among other things, basic linear algebra functions, and advanced random number generation capabilities. Its most powerful feature is the n-dimensional array.\n",
    "\n",
    "- SciPy (scipy): based on NumPy. It is one of the most useful libraries for a wide variety of topics in engineering and science. It implements elements such as the discrete Fourier transform, linear algebra, optimization and sparse matrices.\n",
    "\n",
    "- Matplotlib (matplotlib): used to generate a wide variety of graphics, from histograms to heat maps.\n",
    "\n",
    "- Seaborn (seaborn): very similar to Matplotlib but with some differences in the types of graphs that can be generated.\n",
    "\n",
    "- Pandas (pandas): allows operations on structured data. It is widely used for data manipulation and preparation. Pandas was added to Python and has been instrumental in driving the use of Python in the data science community.\n",
    "\n",
    "To install any of these libraries you should run the command in CMD or Terminal: `pip install lib_name` (or `pip3 install lib_name` depending your set up) where lib_name is the name of the libreary you want to install. For example, to install pandas, you should run:\n",
    "\n",
    "`pip install pandas`\n",
    "\n",
    "To import a library in the code, you should run:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "Then, Pandas can be accessed by **pd**\n",
    "\n",
    "#### 2.2 Data analysis with Pandas\n",
    "\n",
    "Now that we are familiar with the names and objectives of the additional libraries, we will perform a review of data problem solving through Python. In particular, the goal of this section is to build an effective predictive model, which will take us through the following 3 key stages:\n",
    "\n",
    "- Exploratory analysis\n",
    "- Cleaning and debugging\n",
    "- Predictive model building\n",
    "\n",
    "##### 2.2.1 Exploratory analysis\n",
    "\n",
    "For this stage of the process, we will use the Pandas library. In particular, we will use it to read a dataset and perform an exploratory analysis.\n",
    "Pandas has a very informative documentation in this [link](https://pandas.pydata.org/docs/user_guide/index.html#user-guide). You can look for any function to see its definition and some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas works with two main data structures: **DataFrames** and **Series**, which are the basic data models\n",
    "\n",
    "- *Series* can be understood as a one-dimensional labeled/indexed array. If it is labeled, it has similar behavior to a Python built-in dictionary. If it is indexed, it has similar behavior to a Python built-in list. Individual elements of this Series can be accessed through these labels or indexes.\n",
    "\n",
    "- *DataFrame* is similar to an Excel sheet, it has column names that refer to them and it has rows, which can be accessed by using row numbers (or indexes). DataFrames are collection of series, it means, a Series could be a column or a row.\n",
    "\n",
    "Now, we will use for dataset of bank credit information as example. It can be found in the file train.csv. To import it, just use the `read_csv()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We will use pathlib just to control file paths between UNIX and Windows\n",
    "from pathlib import Path\n",
    "\n",
    "# Folder which contains the data\n",
    "data_folder = Path('data')\n",
    "\n",
    "# Reading an external dataset in csv\n",
    "df = pd.read_csv(data_folder/'train.csv')\n",
    "\n",
    "# Showing a sample of the first 10 rows of the database\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a DataFrame is created from an external file (like csv), pandas assume only one data type for each column. It means, if we have only one text value in a column, the entire column will be not numeric. To check this, DataFrames has the `dtypes` attribute which provides the types for each column. If something is not as expected, we can check the format our data and include information about this format in the `read_csv()` function. In the oputput, 'object' means any other complex data as text or above. therefore, here we can check if data is numeric or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing a Serie with all dtypes conteined in the DataFrame\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` method provides the count, mean, variance, minimum, quartiles and maximum of all numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting statistics for numerical values\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, in the DataFrame some columns are not numeric (e.g. Property_Area, Credit_History, etc.). In these situations it is better to use the `value_counts()` method on the desired column. This method counts the total occurrences of each value in the column. In pandas, we use the same syntax of dicts to access to columns and values. It means, `df['Property_Area']` corresponds to the column 'Property_Area'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Property_Area'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking the information present in the describe table:\n",
    "\n",
    "- LoanAmount has (614-592) 22 missing values\n",
    "- Loan_Amount_Term has (614-600) 14 missing values\n",
    "- Credit_History has (614-564) 50 missing values\n",
    "- We can also observe that about 84% of the applicants have a credit history (the mean of the Credit_History field is 0.84).\n",
    "\n",
    "*What should we do with these issues?* (see later...)\n",
    "\n",
    "**A.- Data distribution analysis** \n",
    "\n",
    "Now that we are familiar with the basic characteristics of the data, let us study the distribution of some of its variables. Let us start with the numerical variables `ApplicantIncome` and `LoanAmount`, in particular, with the histogram of `ApplicantIncome` using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ApplicantIncome'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we note that there are some extreme values. This is also the reason why 50 _bins_ are required to clearly represent the distribution.\n",
    "Next, we will review Box Plots to understand the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column = 'ApplicantIncome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot indicates the presence of extreme values more clearly than the histogram. This can be attributed to income disparity in society. Part of this may be driven by the fact that we are looking at people with different levels of education (see the original DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='ApplicantIncome', by = 'Education')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no substantial differences between the average income of graduates and non-graduates. But there are a larger number of graduates with very high incomes, which seem to be the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 4: Data distribution analysis</span> \n",
    "\n",
    "Build an histogram and a boxplot for the variable `LoanAmount`. Try with different number of bins to chose what is the best to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again there are outliers. In order to facilitate subsequent predictive modeling, both `ApplicantIncome` and `LoanAmount` require a certain amount of data cleaning (extreme values are difficult to predict). `LoanAmount` also has incomplete and also extreme values, while `ApplicantIncome` has some extreme values, which require a deeper understanding. We will continue with the analysis of these variables later. Next we will review the analysis of categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.- Categorical variables analysis** \n",
    "\n",
    "Now that we know the distributions for `ApplicantIncome` and `LoanIncome`, let us analyze the categorical variables in more detail. We will use theory from Excel pivot tables and cross tabulation. It is important to note that here loan status has been coded as **Y** for **yes** and **N** for **no**. Therefore, if we consider Y as 1 ans N as 0, the mean represents the probability of obtaining a loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = df['Credit_History'].value_counts(ascending=True)\n",
    "print('Frequency table for credit history:')\n",
    "print(temp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use two advance functionalities of pandas, this two operations come from functional programming.\n",
    "\n",
    "- *lambda*: Lambda is a way to code a function in a variable. It means, the final value comes from apply a function on a certain input.<br>\n",
    "For example:\n",
    "```python\n",
    "        f = lambda x: x+3 # This is the same as the mathematical function f(x)=x+3\n",
    "        f(10)             # If we apply the lambda, we expect an input, and then apply the function\n",
    "        >>> 13\n",
    "```\n",
    "- *pandas map*: It is used to re-assign the values of a Serie based on a logic. In the next cell we use map to assign 1 to Ys and 0 to Ns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = df.pivot_table(values='Loan_Status',index=['Credit_History'],aggfunc = lambda x: x.map({'Y':1,'N':0}).mean())\n",
    "print('Probability of obtaining a loan, based on the existence of credit history:')\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These same tables can be displayed as a bar chart using the *matplotlib* library with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax1 = temp1.plot(kind='bar')\n",
    "ax1.set_xlabel('Credit_History')\n",
    "ax1.set_ylabel('Count of Applicants')\n",
    "ax1.set_title(\"Applicants by Credit_History\")\n",
    "\n",
    "\n",
    "ax2 = temp2.plot(kind = 'bar')\n",
    "ax2.set_xlabel('Credit_History')\n",
    "ax2.set_ylabel('Probability of getting loan')\n",
    "ax2.set_title(\"Probability of getting loan by credit history\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the chances of obtaining a loan are eight times higher if the applicant has a valid credit history.<br>\n",
    "\n",
    "We have just seen how we can do an exploratory analysis in Python using Pandas, which gave us relevant information that will be used in the next stage.\n",
    "\n",
    "##### 2.2.2 Cleaning and debugging\n",
    "\n",
    "As we explore the data, we find some problems in the data, which need to be solved before they are ready to build a model. Here are some of the problems we are already aware of:\n",
    "\n",
    "* Missing values on some variables.\n",
    "* When looking at the distributions, we saw that ApplicantIncome and `LoanAmount` seemed to contain extreme values.\n",
    "\n",
    "In addition to these problems with the numeric fields, we should also look at the non-numeric fields, i.e., `Gender`, `Property Area`, `Married`, `Education`, and `Dependents` to see if they contain useful or incomplete information.\n",
    "\n",
    "**A.- Missing values verification** \n",
    "\n",
    "Let's take a look at the missing values in all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda x: sum(x.isnull()),axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although missing values are not very high in number, many variables have them and each of them must be estimated and aggregated in the data. It is important to note that missing values may not always be NaN or null. Going back to our open question, for instance How to complete missing values in `LoanAmount`?\n",
    "\n",
    "There are numerous ways to fill in the missing values of the loan amount, the simplest being the elimination of rows of missing data. However, a lot of information is often lost. Therefore, replacement by the mean is often the first replacement to try, which can be done using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LoanAmount2'] = df['LoanAmount'].fillna(df['LoanAmount'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take another approach through the following process. First, let's look at the Box Plot to see if there is a trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='LoanAmount', by = ['Education','Self_Employed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see some variation in the median loan amount for each group and this can be used to fill in missing values. But first, we must make sure that each of the `Self_Employed` and `Education` variables should have no missing values. Let's look at the frequency table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Self_Employed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As approximately 86% of the values are \"No\", it is safe to fill in the missing values as \"No\", as there is a high probability of success. This can be done using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Self_Employed'].fillna('No',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a pivot table, which provides us with average values for all single value groups of characteristics `Self_Employed` and `Education`. Next, we define a function, which returns the values of these cells and applies it to fill in the missing values of the loan amount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "table = df.pivot_table(values='LoanAmount', index='Self_Employed' ,columns='Education', aggfunc=np.median)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fage(x):\n",
    "    return table.loc[x['Self_Employed'],x['Education']]\n",
    "\n",
    "df[df['LoanAmount'].isnull()].apply(fage, axis=1)\n",
    "\n",
    "df['LoanAmount'].fillna(df[df['LoanAmount'].isnull()].apply(fage, axis=1), inplace=True)\n",
    "table.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 5: Managing missing values</span> \n",
    "\n",
    "Apply the strategy used to manage missing values of `LoanAmount` on `CreditHistory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.- Extreme values verification** \n",
    "\n",
    "Let's look at LoanAmount first. Since the extreme values are surely not due to an error, it is feasible that some people apply for high loan-to-value loans due to specific needs. So, instead of treating them as outliers, let's try a logarithmic transformation to nullify their effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original distribution\n",
    "df['LoanAmount'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log of Loan amount\n",
    "df['LoanAmount_log'] = np.log(df['LoanAmount'])\n",
    "df['LoanAmount_log'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the distribution looks much closer to a normal (preferable for many predictive models) and the effect of outliers has decreased significantly.\n",
    "\n",
    "Now, regarding `ApplicantIncome`, an intuition may be that some applicants have a lower income, but have strong endorsements. Therefore, it might be a good idea to combine both incomes as total income and take a transformation of this value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original distribution\n",
    "df['ApplicantIncome'].hist(bins=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "df['TotalIncome_log'] = np.log(df['TotalIncome'])\n",
    "df['TotalIncome_log'].hist(bins=20) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 64-bit ('tpm34a')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc97a96f317a709ae2c462a7d0437fc605198aec43f9a7dadb54e6d81820938d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
