{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPM034A Machine Learning for socio-technical systems \n",
    "## `Lab session 02: Artificial Neural Networks`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**Q2 2022**<br>\n",
    "**Instructor:** Sander van Cranenburgh <br>\n",
    "**TAs:**  Francisco Garrido Valenzuela & Lucas Spierenburg <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Instructions`\n",
    "\n",
    "**Lab sessions aim to:**<br>\n",
    "* Show and reinforce how models and ideas presented in class are used in practice.<br>\n",
    "* Help you gather hands-on machine learning skills.<br>\n",
    "\n",
    "**Lab sessions are:**<br>\n",
    "* Learning environments where you work with Jupyter notebooks and where you can get support from TAs and fellow students.<br> \n",
    "* Not graded and do not have to be submitted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Workspace set-up`\n",
    "**Option 1: Google Colab**<br>\n",
    "Uncomment the following cells code lines if you are running this notebook on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/TPM34A/Q2_2022\n",
    "#!pip install -r Q2_2022/requirements_colab.txt\n",
    "#!mv \"/content/Q2_2022/Lab_sessions/lab_session_02/data\" /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Local environment**<br>\n",
    "Uncomment the following cell if you are running this notebook on your local environment. This will install all dependencies on your Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Application: Mode choice prediction` <br>\n",
    "In this lab session, we will use ANN to predict mode-choice behaviour. In transportation, policy-makers often estimate the potential change in mode-choice behaviour resulting from a proposed policy, such as fuel levies, subsidization of public transport, or speed limits. Additionally, they use it for planning transport infrastructure. Based on travel demand forecast e.g. roads are widened or new train connections are built. Traditionally, mode choice predictions are based on classic econometric tools. Recently, the high prediction performance of ML methods appeals to researchers to use ML for predicting choice behaviour too. In this lab session, you will develop ANNs to predict mode behaviour and assess its performance.<br>\n",
    "\n",
    "\n",
    "**Learning objectives**. After completing the following exercises you will be able to: <br>\n",
    "1. `Prepare data` for training\n",
    "2. Train `MultiLayerPerceptron` (MLP) - a particular type of artifical neural network - for a classification task<br>\n",
    "3. `Tune` the `hyperparameters` to improve the model performance<br>\n",
    "4. `Assess the performance` of competing models based on various performance measures, including confusion matrices, and Precision, Recall and Matthew's coefficient<br>\n",
    "\n",
    "#### `Organisation`\n",
    "This lab session comprises **`6`** parts:\n",
    "1. Preparing (choice) data for training Artificial Neural Networks\n",
    "2. Training a MultiLayerPerceptron (MLP) neural network\n",
    "3. Using Early stopping to avoid overfitting\n",
    "4. Using k-fold cross validation to evaluate generalisation performance\n",
    "5. Tuning hyperparameters\n",
    "6. Evaluating performance of trained models\n",
    "\n",
    "and comprises **`7`** `exercises`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python packages and modules\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# Import selected functions and classes from Python packages\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, log_loss, matthews_corrcoef, make_scorer, classification_report\n",
    "\n",
    "# Setting\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preparing (choice) data for training Artificial Neural Networks<br>\n",
    "To prepare the data set, we will:<br>\n",
    "- 1.1 **Load** the data set<br>\n",
    "- 1.2 **Inspect** and **Clean** the data set<br>\n",
    "- 1.3 **Discover and visualise** the data <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Load the choice data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mode choice data into a pandas DataFrame\n",
    "data_folder = Path('data')\n",
    "df = pd.read_csv(data_folder/'swissmetro.dat', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Inspect and clean the data<br>\n",
    "Before starting to analyse your data, make sure you understand what features are in your data.<br>\n",
    "Therefore, it is `highly recommended` to look at the description of the data set. [Click here](data/CS_SwissmetroDescription.pdf) to open the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the range and distribution statistics \n",
    "df.describe()\n",
    "\n",
    "# Inspect the data types in the df\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "# Only keep data for purposes 'Commute\" and \"Business\"\n",
    "df.drop(df[(df.PURPOSE != 1) & (df.PURPOSE != 3)].index, inplace=True) \n",
    "\n",
    "# Drop rows with unknown choices (CHOICE == 0)\n",
    "df.drop(df[df.CHOICE == 0].index, inplace=True) \n",
    "\n",
    "# In case of missing values, we replace them with 0\n",
    "df.fillna(0, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 Discovering and visualising the data<br>\n",
    "Before starting to analyse your data with models, it is advisable to start with some **descriptive analyses**.<br>\n",
    "Therefore, we first look at the distribution and correlations of key features in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 1: Is the data set (im)balanced?</span> \n",
    "`A` Create a histogram showing how often TRAIN, SM and CAR are chosen. Do not forget to add labels to the columns<br>\n",
    "`B` Interpret the the histogram. Is the data set imbalanced? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(8, 4))\n",
    "axes = sns.countplot(x = \"CHOICE\", data = df)\n",
    "ylabels = ['Train', 'SM', 'Car']\n",
    "axes.set_xticklabels(ylabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 2: Explore correlations between features</span> \n",
    "`A` Create a heatmap to identify what features particularly correlate with the **CHOICE**<br>\n",
    "`B` Identify the features that strongly correlate (corr >0.9) (if any). Do they make sense? <br>\n",
    "Hint: to do so you will need the description of the features [Click here](https://github.com/TPM34A/Q2_2022/blob/main/Lab_sessions/lab_session_02/data/CS_SwissmetroDescription.pdf) to open the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training a MultiLayerPerceptron (MLP)\n",
    "To train an MLP (or any other sorts of artifical neural networks) we must take the following steps:<br>\n",
    "- 2.1 **Scaling** the features<br>\n",
    "- 2.2 **Splitting** the data in a training and test data set<br>\n",
    "- 2.3 **Creating the model object**<br>\n",
    "- 2.4 **Training** the model on the train data<br>\n",
    "- 2.5 **Evaluating** the performance on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Scaling the features<br>\n",
    "To efficiently train ANNs it strongly recommended to **scale** (a.k.a. normalise) the features. There are several ways to scale your data. A commonly used scaler of `sk-learn` is called 'StandardScaler'. This scaler normalises the variance and shift the location of the distribution to zero, see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of features that we want to use in the model\n",
    "features = ['PURPOSE', 'FIRST', \n",
    "            'TICKET', 'WHO', 'LUGGAGE', 'AGE', \n",
    "            'MALE', 'INCOME', 'GA', 'ORIGIN', \n",
    "            'DEST', 'TRAIN_TT', 'TRAIN_CO', \n",
    "            'TRAIN_HE', 'SM_TT', 'SM_CO', \n",
    "            'SM_HE', 'SM_SEATS', 'CAR_TT', 'CAR_CO',\n",
    "            'TRAIN_AV', 'SM_AV', 'CAR_AV']\n",
    "X = df.loc[:,features]\n",
    "\n",
    "# Initiate scaler object & fit to data\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X)  \n",
    "\n",
    "# Create new dataframe X_scaled containg the scaled features AND the (unscaled) ones: availabilities, Group, Survey, Choice\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Create the target\n",
    "Y = df['CHOICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Splitting the data in a train set and a test set<br>\n",
    "Training ML models always involves a **train** and a **test** data set. The train set is used to update the weights of the model. That test set is used to evaluate the **generalisation performance** of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data using sk-learn's `train_test_split` function\n",
    "# Note that we use 60% for training and 40% for testing\n",
    "# Note that we set the random_state, in order to replicate results later (do not change) \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, random_state = 12345, test_size = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Creating the model object<br>\n",
    "A MultiLayerPerceptron (MLP) is a fully-connected feed-forward neural network. We create the MLP using `sk-learn's` MLPClassifier function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'plain vanilla' MLP object\n",
    "# Declare the number of layers and nodes per layer\n",
    "# layers = (a,b) means two layer with a nodes in the 1st hidden layer and b nodes in the 2nd hidden layer\n",
    "layers = (10)\n",
    "\n",
    "# Define MLP architecture, optimiser and hyperparameters:\n",
    "# We use Adam optimiser\n",
    "# We use the learning rate to 0.001\n",
    "# We use L2 regularisation of 0.1\n",
    "# We use a batch size of 250 observations\n",
    "# We use tanhg activation (transfer function)\n",
    "# We set the max number of epochs to 2000\n",
    "mlp = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = 0.001, alpha=0.1, batch_size=250, activation = 'tanh', max_iter = 2000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4 Training the model<br>\n",
    "To train the MLP we use the '.fit' function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MLP using the train data.\n",
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is good practice to look at the learning curve, to see how much the model has improved from the initial point,\n",
    "# and how is has evolved over epochs.\n",
    "fig, ax = plt.subplots(figsize = (16,8))\n",
    "plt.plot(mlp.loss_curve_)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Cross entropy loss')\n",
    "ax.grid(True,linewidth = 0.5)\n",
    "ax.set_ylim(0.0,1.2)\n",
    "ax.set_xlim(0,mlp.n_iter_)\n",
    "ax.set_title(f'Cross entropy loss on the TRAINING DATA. \\nBest CE = {mlp.loss_:4.3f}')\n",
    "# The plot illustrates that the training stops when the training loss does no longer improve more that a given tolerance (e.g. 1e-6), or reaches max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5 Evaluating the model performance<br>\n",
    "To evaluate the generalisation performance of our MLP, we look at the accuracy and cross-entropy on the train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function that returns the accuracy and the cross entropy, for the train and test data sets\n",
    "def calculate_acc_ce(mlp,X_train,Y_train,X_test, Y_test):\n",
    "    \n",
    "    def calculate_acc(mlp,X,Y):\n",
    "        accuracy = mlp.score(X,Y)\n",
    "        return accuracy\n",
    "\n",
    "    def calculate_ce(mlp,X,Y):\n",
    "        # Compute cross entropy\n",
    "        # Use the model object to predict probabilities per class\n",
    "        prob = mlp.predict_proba(X)\n",
    "\n",
    "        # Multiply the probabilities with Y (0/1 array), and sum along the row axis to obtain the predicted probability of the target\n",
    "        Y_dummy = pd.get_dummies(Y).to_numpy()\n",
    "        prob_chosen = np.sum(prob*Y_dummy,axis=1)    \n",
    "        \n",
    "        # Take the logarithm\n",
    "        log_prob_chosen = np.log(prob_chosen)\n",
    "\n",
    "        # Compute the cross entropy\n",
    "        cross_entropy = -np.sum(log_prob_chosen)/len(Y)\n",
    "        return cross_entropy\n",
    "\n",
    "    # Compute the accuracy\n",
    "    acc_train = calculate_acc(mlp,X_train,Y_train)\n",
    "    acc_test  = calculate_acc(mlp,X_test,Y_test)\n",
    "\n",
    "    # Apply cross entropy function\n",
    "    ce_train = calculate_ce(mlp,X_train,Y_train)\n",
    "    ce_test = calculate_ce(mlp,X_test,Y_test)\n",
    "    return acc_train, acc_test, ce_train, ce_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply our evaluation function\n",
    "accuracy_train, accuracy_test, cross_entropy_train, cross_entropy_test = calculate_acc_ce(mlp,X_train,Y_train,X_test, Y_test)\n",
    "\n",
    "# Print the results\n",
    "print('\\t\\t Train set\\t Test set')\n",
    "print(f'Accuracy\\t {accuracy_train:0.3f}\\t\\t {accuracy_test:0.3f}')\n",
    "print(f'Cross entropy\\t {cross_entropy_train:0.3f}\\t\\t {cross_entropy_test:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 3: Does using more nodes improve the model performance?</span> \n",
    "`A` Calculate the number of weights consumed by the current MLP with 10 hidden nodes in 1 hidden layer. <br>\n",
    "`B` Retrain your model several times with {10,30,60,90} nodes. Report the cross-entropy performance on the train and test data sets.<br>\n",
    "`C` Does increasing the number of nodes lead to a lower cross-entropy on the train and or test set? What is happening?<br>\n",
    "`D` Suppose you would have many more choice observations from this survey. Would that enable you to develop a much better model with a cross-entropy performance of say <0.10 (on the test set)? Explain your answer. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Using Early stopping to avoid overfitting\n",
    "Early stopping refers to a technique that stops the training of the network when the performance on the test data set no longer improves. Thereby, early stopping avoids the model to overfit the data. It essentially stops the training before the model can overfit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this is the same 'plain vanilla' MLP, but now we set early_stopping = True\n",
    "# A validation_fraction is added. This fraction is the proportion of training data to set aside as validation set for early stopping. This data set is used to determine when to stop. \n",
    "# The training stops when the performance on the validation data set does not improve for n_iter_no_change in a row\n",
    "# We use the MLP with 10 nodes and one hidden layer again.\n",
    "layers = (10)\n",
    "n_iter_no_change = 10\n",
    "mlp_early_st = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = 0.001, alpha=0.1, batch_size=250, activation = 'tanh', max_iter = 2000, early_stopping=True, n_iter_no_change = n_iter_no_change,validation_fraction = 0.25) \n",
    "\n",
    "# Train the MLP using the train data\n",
    "mlp_early_st.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is good practice to look at the learning curve, to see how much the model has improved from the initial point,\n",
    "# and how is has evolved over epochs.\n",
    "# The learning curve plot also illustrates well what early stopping does.\n",
    "fig, ax = plt.subplots(figsize = (16,8))\n",
    "plt.plot(mlp.loss_curve_,label=f'MLP with {mlp.hidden_layer_sizes} hidden nodes, WITHOUT early stopping')\n",
    "plt.plot(mlp_early_st.loss_curve_,label=f'MLP with {mlp_early_st.hidden_layer_sizes} hidden nodes, WITH early stopping')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Cross entropy loss')\n",
    "ax.grid(True,linewidth = 0.5)\n",
    "ax.set_ylim(0.0,1.2)\n",
    "ax.set_xlim(0,mlp.n_iter_)\n",
    "ax.set_title('Effect of early stopping')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the performance of the MLP with early stopping using our evaluation function\n",
    "accuracy_train_early_st, accuracy_test_early_st, cross_entropy_train_early_st, cross_entropy_test_early_st = calculate_acc_ce(mlp_early_st,X_train,Y_train,X_test, Y_test)\n",
    "\n",
    "# Report results\n",
    "print('\\t\\t Train set\\t Test set')\n",
    "print(f'Accuracy\\t {accuracy_train_early_st:0.3f}\\t\\t {accuracy_test_early_st:0.3f}')\n",
    "print(f'Cross entropy\\t {cross_entropy_train_early_st:0.3f}\\t\\t {cross_entropy_test_early_st:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 4: Can we overfit with early stopping?</span> \n",
    "`A` Did early stopping reduced overfitting? How can you see this from the results?<br>\n",
    "`B` Try if early stoppping also helps to avoid overfitting when using more nodes, e.g. 90 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Using k-fold cross validation to evaluate generalisation performance\n",
    "k-fold cross validation is commonly used to more accurately **evaluate the generalisation performance** of a given network. It improves a simple train-test split approach in that it systematically cuts the data set in k pieces. k-fold cross validation is especially crucial when tuning hyperparameters (as we will see later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP object (plain vanilla MLP without early stopping)\n",
    "layers = 10\n",
    "mlp_cv = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = 0.001, alpha=0.1, batch_size=250, activation = 'tanh', max_iter = 2000) \n",
    "\n",
    "# Create scoring function\n",
    "# It is necessary to create a scoring function when working with cross_validate of sk-learn\n",
    "# We set `greater_is_better` to `False` as we are minimising cross entropy loss\n",
    "logloss = make_scorer(log_loss, greater_is_better = False, needs_proba = True)\n",
    "\n",
    "# Apply cross_validate, using e.g. 5 folds\n",
    "# Since we use cross-validation training takes n_folds times longer than using a train-test split\n",
    "n_folds = 5\n",
    "cv_results = cross_validate(mlp_cv,X_scaled,Y,cv = n_folds, scoring=logloss,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train and test performance in a bar plot, for each fold\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_axis = np.arange(n_folds)\n",
    "ax.bar(x_axis + -0.125, -cv_results['train_score'], color = 'b', width = 0.25,label = 'Train data set')\n",
    "ax.bar(x_axis +  0.125, -cv_results['test_score'], color = 'g', width = 0.25,label = 'Test data set')\n",
    "ax.set_xlabel('fold #')\n",
    "ax.set_ylabel('cross entropy')\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Hence, this cross validation analysis supports the finding that a plain vanilla MLP (i.e. without early stopping) overfits the data considerably}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Tuning hyperparameter\n",
    "When training MLPs (and most other ML models) there are several parameters we can **'tune'** (optimise) to improve the model's performance. The process of doing this is called **hyperparameter tuning**. Hyperparameter tuning can be done manually, but that is unwieldy. The `GridSearchCV` function in `sk-learn` automates the hyperparameter tuning process. When tuning the hyperparameters, it is mandatory to use a k-fold cross validation approach. Otherwise, there is a risk of overfitting on the test set *because* the parameters can be tweaked until the estimator performs optimally on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP object (plain vanilla MLP)\n",
    "mlp_gs = MLPClassifier(activation = 'tanh', solver='adam', batch_size=250, max_iter=2000)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "# 'hidden_layer_sizes' defines the number of nodes and layers\n",
    "# 'alpha' governs the L2 regularisation\n",
    "# 'learning_rate_init' governs the learning rate.\n",
    "hyperparameter_space = {\n",
    "    'hidden_layer_sizes': [(10),(10,10),(30,30)],\n",
    "    'alpha': [1,1e-2,1e-4],\n",
    "    'learning_rate_init': [0.01,0.001,0.0001]}\n",
    "\n",
    "# Create scoring function\n",
    "logloss = make_scorer(log_loss, greater_is_better = False, needs_proba = True)\n",
    "\n",
    "# Create the grid_search object, with using the MLP classifier\n",
    "folds = 5 # Number of cross validation splits\n",
    "mlp_gridsearch = GridSearchCV(mlp_gs, hyperparameter_space, n_jobs=-1, cv=folds,scoring = logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the training/gridsearch\n",
    "# Note that this is computationally expensive! \n",
    "# It may take up to 5 minutes, since 3 x 3 x 3 = 27 models need to be trained, each with 5 folds (=135)\n",
    "mlp_gridsearch.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model \n",
    "filename = 'my_tuned_model.sav'\n",
    "pickle.dump(mlp_gridsearch, open(data_folder/filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model (if you have a saved model)\n",
    "# mlp_gridsearch = pickle.load(open(data_folder/filename,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the hyperparameter tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results into a new pandas dataframe\n",
    "df_gridsearch = pd.DataFrame.from_dict(mlp_gridsearch.cv_results_)\n",
    "    \n",
    "# Add new column with a label for the hyperparameter combinations %% NOT SURE IF THIS IS BET WAY TO DO THIS IN PYTHON   \n",
    "df_gridsearch['gs_combinations'] = 'L2 = '+ df_gridsearch['param_alpha'].astype('str') + '; Learning_rate = '+ df_gridsearch['param_learning_rate_init'].astype('str') + '; Layers = ' + df_gridsearch['param_hidden_layer_sizes'].astype('str')\n",
    "df_gridsearch = df_gridsearch.sort_values('rank_test_score')\n",
    "\n",
    "# Visualise deviation in performance across hyper parameter settings\n",
    "plt.figure(figsize = (16,6))\n",
    "ax = sns.barplot(x = df_gridsearch.gs_combinations,y=-df_gridsearch.mean_test_score,palette=\"Blues_d\",)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "print('Best hyperparameters found:\\t', mlp_gridsearch.best_params_)\n",
    "print('Best model performance:\\t\\t', -mlp_gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 5:  Hyperparameter tuning</span> \n",
    "`A` What hyperparameter turns out to be particularly impactful on the model performance?<br>\n",
    "`B` Can you think of reasons why this could be the case?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Re)Training the model with optimised hyperparameters**<br> \n",
    "After completing hypertuning, you know the optimal hyperparameters. <br>\n",
    "Therefore, after hypertuning we always retrain the model using the optimised hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new mlp object using the optimised hyperparameters, just using the train/test split\n",
    "layers = mlp_gridsearch.best_params_['hidden_layer_sizes']\n",
    "lr = mlp_gridsearch.best_params_['learning_rate_init']\n",
    "alpha = mlp_gridsearch.best_params_['alpha']\n",
    "mlp_gs = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = lr, alpha=alpha, batch_size=250, activation = 'tanh', max_iter = 2000) \n",
    "\n",
    "# Train the model\n",
    "mlp_gs.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also evaluate performance of the hypertuned model using our evaluation function\n",
    "accuracy_train_gs, accuracy_test_gs, cross_entropy_train_gs, cross_entropy_test_gs = calculate_acc_ce(mlp_gs,X_train,Y_train,X_test, Y_test)\n",
    "\n",
    "# Report results\n",
    "print('\\t\\t Train set\\t Test set')\n",
    "print(f'Accuracy\\t {accuracy_train_gs:0.3f}\\t\\t {accuracy_test_gs:0.3f}')\n",
    "print(f'Cross entropy\\t {cross_entropy_train_gs:0.3f}\\t\\t {cross_entropy_test_gs:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Evaluating and comparing performances across trained models\n",
    "\n",
    "To evaluate the performance of a wide range is metrics are available, beyond generalisation performance.<br>\n",
    "Here, we look at:<br>\n",
    "i. Confusion matrix<br>\n",
    "ii. Precision and Recall<br>\n",
    "iii. Matthew's correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i) Confusion matrix**<br>\n",
    "Confusion matrices shows counts from predicted and actual outcomes. The counts on the diagonal are correctly classified outcomes (the model predictions and the ground truth are the same). The counts on the off-diagonal elements are the misclassified outcomes. Hence, the best classifier will have a confusion matrix with only diagonal elements and the rest of the elements set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the choices for the test data set, using the MLP trained with early stopping and MLP with hyperparameters tuned\n",
    "Y_pred_early_st  = mlp_early_st.predict(X_test)  # 0/1 predictions of MLP trained with early stopping\n",
    "Y_pred_gs = mlp_gs.predict(X_test)                      # 0/1 predictions of MLP with hyperparameters tuned\n",
    "\n",
    "# Show the confusion matrices, to compare the hyperparameter tuned network with the early stopping network\n",
    "fig, ax = plt.subplots(2,2,figsize = (16,12))\n",
    "fig.set_tight_layout(True)\n",
    "ylabels = ['Train', 'SM', 'Car']\n",
    "cm1 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_early_st, display_labels = ylabels, normalize=None,  ax=ax[(0,0)])\n",
    "cm2 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_early_st, display_labels = ylabels, normalize='true',ax=ax[(1,0)])\n",
    "cm3 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_gs, display_labels = ylabels, normalize=None,  ax=ax[(0,1)])\n",
    "cm4 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_gs, display_labels = ylabels, normalize='true',ax=ax[(1,1)])\n",
    "\n",
    "# Add titles\n",
    "cm1.ax_.set_title(f'MLP with {mlp_early_st.hidden_layer_sizes} nodes \\n trained with early stopping')\n",
    "cm2.ax_.set_title(f'MLP with {mlp_early_st.hidden_layer_sizes} nodes \\n trained with early stopping')\n",
    "cm3.ax_.set_title(f'MLP with {mlp_gs.hidden_layer_sizes} nodes \\n hyperparameters tuned')\n",
    "cm4.ax_.set_title(f'MLP with {mlp_gs.hidden_layer_sizes} nodes \\n hyperparameters tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 6:  Model accuracy</span> \n",
    "Accuracy is defined as the true positives over the total number of cases.<br>\n",
    "`A` Manually compute the prediction accuracy of the model with early stopping and the model with hyperparameter tuning<br>\n",
    "`B` For which class (Train/SM/Car) does the hyperparameter tuning improves the prediction accuracy most?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii) Precision and Recall**<br>\n",
    "Looking at the confusion matrices, the improvements in prediction accuracy due to the hyperparameter tuning may not seem very impressive. However, one should keep in mind that 0/1 predictions are sensitive to class imbalances, which are present in these data. Moreover, accuracy can be a misleading metric for imbalanced data sets. A naive model that would simply always predict \"SM\" will already do quite good.\n",
    "\n",
    "To assess the model performance in more depth, we thus must look at the predictions at the level of the classes.<br> \n",
    "Next, we compute Precision and Recall.<br>\n",
    "* **Precision** Tells you what fraction of predictions for a given class are actually of that class.<br>\n",
    "* **Recall** Tells what fraction of all observations belonging to a given class are correctly predicted as such by the model. Recall is also known as True Positive Rate (TPR), Sensitivity, Probability of Detection. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the precision and recal score we conveniently use sk-learn's 'classification_report' functionality\n",
    "print('Classification report for plain vanilla MLP with early stopping\\n',classification_report(Y_test,Y_pred_early_st, target_names= ylabels))\n",
    "print('\\nClassification report for plain vanilla MLP with hyperparameters tuned\\n',classification_report(Y_test,Y_pred_gs, target_names= ylabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii) Matthew's correlation coefficient**<br>\n",
    "Another commonly used metric to evaluate the prediction performance while accounting for imbalances in the data set is  Matthew's correlation coefficient. Matthew's Correlation Coefficient (MCC) is generally regarded as being one of the best measures to describe the confusion matrix of true and false positives and negatives by a **single number**, even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking into account the imbalances of the data\n",
    "print(f'Matthews correlation coefficient for plain Vanilla MLP with early stopping:\\t {matthews_corrcoef(Y_test, Y_pred_early_st):4.3f}')\n",
    "print(f'Matthews correlation coefficient for plain Vanilla MLP with hypertuning:\\t {matthews_corrcoef(Y_test, Y_pred_gs):4.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 7:  Model precision, recall, and Matthew's correlation coefficent</span> \n",
    "`A` Compare and interpret the results from the classifications reports between the early stopping and hypertuned model.<br>\n",
    "`B` Compare and interpret the results from Matthew's correlation coefficient between the early stopping and hypertuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 64-bit ('tpm34a')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc97a96f317a709ae2c462a7d0437fc605198aec43f9a7dadb54e6d81820938d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
